# ğŸ§  XAI-PGM-Counterfactuals: Explainable AI with Probabilistic Graphical Models

## ğŸ“Œ Overview
This project presents a **novel Explainable AI (XAI) framework** integrating **Convolutional Neural Networks (CNNs)** and **Probabilistic Graphical Models (PGMs)** to provide **counterfactual explanations** for AI-driven medical diagnoses. The methodology focuses on **interpreting CNN predictions** in a **pneumonia classification task** using **chest X-ray images**, enhancing transparency in AI-driven decision-making.

### **ğŸ”¹ Key Contributions**
âœ” **CNN Feature Extraction:** Identifies **critical regions** in medical images.  
âœ” **Prototype-Based Explainability:** Maps CNN **feature maps** to interpretable regions.  
âœ” **Bayesian Networks:** Models relationships **between image regions**.  
âœ” **Counterfactual Reasoning:** Evaluates **alternative explanations** for AI predictions.  
âœ” **Comparison with LIME, SHAP, and Grad-CAM:** Benchmarks performance with existing XAI techniques.

---

## ğŸ“‚ Dataset
- **Dataset Used:** [Chest X-ray Images (Pneumonia)](https://www.kaggle.com/datasets/paultimothymooney/chest-xray) ğŸ“¸  
- **Task:** **Binary classification** (Normal vs. Pneumonia).
- **Feature Representation:** CNN-extracted features from **segmented lung regions**.

---

## ğŸš€ Methodology
### **1ï¸âƒ£ CNN Feature Extraction**
- A **Convolutional Neural Network (CNN)** is trained on chest X-ray images.
- Extracted **feature maps** from the **last convolutional layer**.
- Divided images into a **3Ã—3 grid** for **localized interpretability**.
- **Cosine similarity** is used to compare regions with **prototypes**.

### **2ï¸âƒ£ Prototype-Based Explainability**
- **K-Means Clustering** applied to extracted features for **regional prototypes**.
- **Principal Component Analysis (PCA)** used for feature reduction.
- Similarity between new images and stored prototypes computed using **cosine similarity**.

### **3ï¸âƒ£ Probabilistic Graphical Model (PGM) Construction**
- **Bayesian Network (BN)** created where **nodes represent image regions**.
- BN structure **learned using Hill Climb Search + K2 Score**.
- Conditional Probability Distributions (CPDs) estimated using **Maximum Likelihood Estimation**.

### **4ï¸âƒ£ Counterfactual Generation & Analysis**
- **Region-wise feature similarities** in the Bayesian Network are **modified**.
- Evaluated how **changes in individual regions** affect CNN predictions.
- **Spearmanâ€™s Rank Correlation (Ï) and Weighted Spearmanâ€™s Rank Correlation (WFMÏ)** used to **validate counterfactual results**.
- Counterfactuals compared with **LIME, SHAP, and Grad-CAM**.


## Basic Visuals

##  Bayesian Network Structure
This figure represents the **learned Bayesian Network (BN)** used for structured interpretability.
![Bayesian Network](images/BN.png)

##  Training & Validation Loss Over Epochs
The **CNN model's training and validation performance**:
![Training & Validation](images/Training_Validation.png)

## Random Image divided into 9 regions
![Dived_Image](images/Xray_divided.png)

##  LIME Explanation â€“ Feature Importance
LIME results showing **which image regions influenced the CNN decision**.
![LIME Explanation](images/LIME.png)

##  SHAP Explanation â€“ Feature Attribution
SHAP heatmap indicating **the impact of each region on model prediction**.
![SHAP Explanation](images/SHAP.png)

##  Grad-CAM Heatmap â€“ Model Focus
Grad-CAM visualization showing **which image regions the CNN focused on**.
![Grad-CAM Explanation](images/GradCAM.png)

---

## ğŸ“Š Results & Key Findings

### **Counterfactual Explanations for CNN Predictions**
| Changed Region | Change (From â†’ To) | Original Prob | New Prob | Change in Prob |
|---------------|--------------------|--------------|----------|---------------|
| **Region 7**  | Medium â†’ High      | 52.53%       | 84%      | +31.34%       |
| **Region 7**  | Medium â†’ Low       | 52.53%       | 26%      | -26.84%       |
| **Region 4**  | Medium â†’ Low       | 52.53%       | 29%      | -23.16%       |
| **Region 0**  | Low â†’ Medium       | 52.53%       | 68%      | +15.50%       |
| **Region 4**  | Medium â†’ High      | 52.53%       | 66%      | +13.72%       |

## ğŸ” Comparison with Other Explainability Techniques

To validate the effectiveness of **PGM-based counterfactual explanations**, we compare the results with **three well-known XAI techniques**:
- **Grad-CAM** (Gradient-weighted Class Activation Mapping)
- **LIME** (Local Interpretable Model-agnostic Explanations)
- **SHAP** (Shapley Additive Explanations)

The evaluation is based on:
1. **Spearmanâ€™s Rank Correlation (Ï):** Measures how well-ranked importance scores align.
2. **Weighted Spearmanâ€™s Rank Correlation (WFMÏ):** Adjusts for feature importance weighting.
3. **p-values:** Statistical significance of correlation results.

### **ğŸ“Š Quantitative Evaluation**
| Method         | Ï (Spearman) | p-value (Ï) | WFMÏ | p-value (WFMÏ) |
|---------------|-------------|-------------|------|----------------|
| **LIME**      | 0.133       | 0.733       | 0.8973 | 0.001 |
| **SHAP**      | 0.300       | 0.4328      | 0.9568 | 0.0001 |
| **Grad-CAM**  | **0.7667**  | **0.0159**  | **0.9899** | **0.0001** |

### **ğŸ”¹ Key Insights**
âœ” **Grad-CAM exhibits the highest correlation (Ï = 0.7667) with PGM Counterfactuals**, confirming strong alignment in region importance.  
âœ” **LIME shows weak correlation (Ï = 0.133), with a high p-value (0.733), indicating low statistical significance**.  
âœ” **SHAP performs moderately well (Ï = 0.3), with WFMÏ = 0.9568, but lower statistical significance compared to Grad-CAM**.  
âœ” **Weighted correlations (WFMÏ) are significantly higher for all methods, highlighting the impact of feature importance weighting**.  

---

## ğŸ“œ Final Thoughts
This comparison confirms that **PGM Counterfactuals provide a structured, probabilistic approach** to CNN explainability, aligning well with existing techniques while offering **causal interpretability**.

---

